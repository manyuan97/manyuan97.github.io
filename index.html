<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Manyuan Zhang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="cv.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Manyuan Zhang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="bioo.jpg" alt="alt text" width="160px" height="160px" />&nbsp;</td>
<td align="left"><p>Ph.D. candidate<br /> <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory</a><br /><a href="http://www.ee.cuhk.edu.hk/en-gb/">Department of Electornic Engineering</a> <br /><a href="https://www.cuhk.edu.hk/chinese/index.html">The Chinese University of Hong Kong</a><br />
Office: SHB 304, CUHK, Hong Kong S.A.R., China <br />
Email: zhangmanyuan@link.cuhk.edu.hk <br />
<a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=ZYmcm0EAAAAJ">Scholar</a> <a href="cv.pdf">CV</a> <a href="https://github.com/manyuan97">Github</a> <a href="https://www.linkedin.com/in/manyuan-zhang-32997814b/?originalSubdomain=cn">Linkin</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I am currently a Ph.D. candidate at <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Lab (MMLab)</a>, the Chinese University of Hong Kong. I am supervised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof.Hongsheng Li</a> and  <a href="https://www.ee.cuhk.edu.hk/~xgwang/">Prof.Xiaogang Wang</a>. I received my bachelor's degree in Network Engineering from the School of Information  and Communication Engineering at <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in 2019. Now, I am also a Researcher  at <a href="https://www.sensetime.com/cn/">SenseTime Research</a>, working closely with  <a href="http://liuyu.us/">Yu Liu</a> and <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&amp;hl=zh-CN/">Guanglu Song</a>.<br /><br />
In my more than five-year career at the <a href="https://www.sensetime.com/cn/">SenseTime Research</a>, I have been involved in many projects from scratch. We built the most reliable face recognition system in the world at that time (the champion of <a href="https://pages.nist.gov/frvt/reportcards/11/sensetime_005.html">FRVT</a>, <a href="https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/">ICCV MFR</a>), the best video recognition model (the champion of ActivityNet Challenge <a href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Kinetics700</a> ), replicated the AI of StarCraft2 (<a href="https://github.com/opendilab/DI-star">DI-star</a>) from scratch, developed an autonomous driving algorithm based on reinforcement learning (<a href="https://github.com/opendilab/DI-drive">DI-drive</a>), and most recently, the text-to-image AIGC product <a href="https://miaohua.sensetime.com/">SenseMirage</a>. If you are interested in my work or career, please feel free to contact me.</p>
<h2>News</h2>
<ul>
<li><p>[2023-07] Two paper accepted to ICCV2023.</p>
</li>
<li><p>[2023-07] I pass the PhD candidate test. </p>
</li>
<li><p>[2023-05] I am invited to be a reviewer for NIPS2023 and ICLR2023.</p>
</li>
<li><p>[2023-02] One paper accepted to CVPR 2023.</p>
</li>
<li><p>[2022-12] I am invited to be a reviewer for CVPR2023 and ICCV2023.</p>
</li>
<li><p>[2022-07] One paper accepted to ECCV 2022.</p>
</li>
<li><p>[2022-04] I am invited to be a reviewer for ECCV2022 and NIPS2022.</p>
</li>
<li><p>[2022-04] I am invited to &rsquo;智东西&rsquo; to give a talk about imitation learning in automatic driving.</p>
</li>
<li><p>[2021-10] We win three championships of <a href="https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/">ICCV 2021 Masked Face Recognition Challenge</a> on <a href="http://iccv21-mfr.com/">glink360k</a> track, <a href="http://iccv21-mfr.com/">unconstrained</a> track and <a href="https://competitions.codalab.org/competitions/32478">Webface260M</a> track. Code and solutions will be released very soon.</p>
</li>
<li><p>[2021-07] We release  <a href="https://github.com/opendilab/DI-drive/">DI-drive</a>, the decision intelligence platform for autonomous driving simulation. I am responsible for the imitation learning part.</p>
</li>
<li><p>[2021-07] One paper accepted to ICCV 2021.</p>
</li>
<li><p>[2021-05] We win the championship of <a href="https://pages.nist.gov/frvt/reportcards/11/sensetime_005.html">NIST FRVT 1:1</a>.</p>
</li>
<li><p>[2020-12] We win the championship of <a href="https://pages.nist.gov/frvt/reportcards/1N/sensetime_005.pdf">NIST FRVT 1:N</a>.</p>
</li>
<li><p>[2020-06] We win 2 championships of <a href="http://activity-net.org/">ActivityNet</a> on the <a href="http://activity-net.org/challenges/2020/tasks/guest_ava.html/">Spatio-temporal Action Localization (AVA)</a> track and the <a href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Trimmed Activity Recognition (Kinetics 700)</a> track. </p>
</li>
<li><p>[2020-06] One paper accepted to ECCV 2020.</p>
</li>
<li><p>[2020-04] We release the <a href="https://github.com/Sense-X/X-Temporal/">X-Temporal</a> for easily implement SOTA video understanding methods with PyTorch on multiple machines and GPUs.</p>
</li>
<li><p>[2019-10] One paper accepted to ICCV 2019 LFR workshop.</p>
</li>
<li><p>[2019-10] We win the championship of <a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">ICCV19 Multi-Moments in Time (MIT) Challenge</a>.</p>
</li>
<li><p>[2019-10] We win the championship of <a href="https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/">ICCV19 Lightweight Face Recognition Challenge</a>.</p>
</li>
</ul>
<h3>Recent Publications </h3>
<p>*equal contribition</p>
<ul>
<li><p>Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection <br />
<b>Manyuan Zhang</b>, Guanglu Song, Yu Liu, Hongsheng Li <br />
2023  International Conference on Computer Vision (ICCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2303.08340">VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation</a> <br />
Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, <b>Manyuan Zhang</b>, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li <br />
2023  International Conference on Computer Vision (ICCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2303.01237">FlowFormer<tt></tt>: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation</a> <br />
Xiaoyu Shi, Zhaoyang Huang, Dasong Li, <b>Manyuan Zhang</b>, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li <br />
2022 The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) </p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.13600/">Towards Robust Face Recognition with Comprehensive Search</a> <br />
<b>Manyuan Zhang</b>, Guanglu Song, Yu Liu, Hongsheng Li <br />
2022 European Conference on Computer Vision (ECCV)</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Switchable_K-Class_Hyperplanes_for_Noise-Robust_Representation_Learning_ICCV_2021_paper.pdf/">Switchable K-class Hyperplanes for Noise-robust Representation Learning</a> <br /> 
Boxiao Liu, Guanglu Song, <b>Manyuan Zhang</b>, Haihang You, Yu Liu <br />
2021 International Conference on Computer Vision (ICCV)</p>
</li>
<li><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550001.pdf/">Discriminability Distillation in Group Representation Learning</a> <br />
<b>Manyuan Zhang</b>, Guanglu Song, Hang Zhou, Yu Liu <br />
2020 European Conference on Computer Vision (ECCV)</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Liu_Towards_Flops-Constrained_Face_Recognition_ICCVW_2019_paper.pdf/">Towards Flops-constrained Face Recognition</a> <br />
Yu Liu*, Guanglu Song*, <b>Manyuan Zhang*</b>, Jihao Liu*, Yucong Zhou, Junjie Yan <br />
2019 ICCV Lightweight Face Recognition Challenge &amp; Workshop</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/8486609">Tensor sensing for RF tomographic imaging</a> <br />
Tao Deng, Feng Qian, Xiao-Yang Liu, <b>Manyuan Zhang</b>, Anwar Walid <br /> 
2018 IEEE International Conference on Multimedia and Expo (ICME) </p>
</li>
<li><p><a href="https://arxiv.org/pdf/1803.10943.pdf/">Privacy-preserving sensory data recovery</a> <br />
Cai Chen, <b>Manyuan Zhang</b>, Huanzhi Zhang, Zhenyun Huang, Yong Li <br />
2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications <br /></p>
</li>
</ul>
<h3>Challenge Awards</h3>
<ul>
<li><p>Won the <b>1th</b> place in CVPR21 Masked Face Recognition Challenge (<a href="https://www.face-benchmark.org/challenge.html/">WebFace260M</a>, <a href="https://insightface.ai/mfr21/">InsightFace Unconstrained</a> and 
<a href="https://insightface.ai/mfr21">InsightFace glint360k</a> track)</p>
</li>
<li><p>Won the <b>1th</b> place in CVPR20 ActivityNet Challenge (<a href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Kinetics700</a> track and  <a href="https://www.youtube.com/watch?v=zJPEmG3LCH4&amp;list=PLw6H4u-XW8siSxqdRVcD5aBn3OTuA7M7x/">AVA track</a>)</p>
</li>
<li><p>Won the <b>1th</b> place in NIST FRVT held by US government (<a href="https://github.com/usnistgov/frvt/blob/nist-pages/reports/11/frvt_11_report_2022_07_29.pdf">1:1 Verification</a> and <a href="https://github.com/usnistgov/frvt/blob/nist-pages/reports/1N/frvt_1N_report_2022_07_28.pdf/">1:N Identification</a>)</p>
</li>
<li><p>Won the <b>1th</b> place in <a href="http://moments.csail.mit.edu/results2019.html/">ICCV19 Multi-Moments in Time (MIT) Challenge</a></p>
</li>
<li><p>Won the <b>1th</b> place in <a href="https://insightface.ai/lfr19/">ICCV19 Lightweight Face Recognition Challenge</a></p>
</li>
</ul>
<h3>Technical Report </h3>
<ul>
<li><p><a href="Towards">Large-scale Masked Face Recognition</a> (Top-1 Solution) <br />
<b>Manyuan Zhang</b>, Bingqi Ma, Guanglu Song, Yunxiao Wang, Hongsheng Li, Yu Liu</p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.09116/">1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020</a>  (Top-1 Solution) <br />
Siyu Chen, Junting Pan, Guanglu Song, <b>Manyuan Zhang</b>, Hao Shao, Ziyi Lin, Jing Shao, Hongsheng Li, Yu Liu </p>
</li>
<li><p><a href="https://arxiv.org/abs/2003.05837/">Top-1 Solution of Multi-Moments in Time Challenge 2019</a>  (Top-1 Solution) <br />
<b>Manyuan Zhang</b>, Hao Shao, Guanglu Song, Yu Liu, Junjie Yan</p>
</li>
</ul>
<h3>Selected Projects</h3>
<ul>
<li><p><a href="https://github.com/Sense-X/X-Temporal/">X-Xemporal</a> <br />
Easily implement SOTA video understanding methods with PyTorch on multiple machines and GPU.</p>
</li>
<li><p><a href="https://github.com/opendilab/DI-drive/">DI-drive</a> <br /> 
Decision Intelligence Platform for Autonomous Driving simulation.</p>
</li>
</ul>
<h3>Working Experience</h3>
<ul>
<li><p>Research intern at <a href="https://www.sensetime.com/cn/">SenseTime Research</a> (since Feb 2019) <br />
Working on large-scale face recognition and video understanding with <a href="http://liuyu.us/">Yu Liu</a> and <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&amp;hl=zh-CN/">Guanglu Song</a></p>
</li>
<li><p>Research intern at <a href="https://en.megvii.com/megvii_research/">Megvii Research</a> (from Aug 2018 to Feb 2019) <br />
Working on style transfer with <a href="http://www.liushuaicheng.org/">Shuaicheng Liu</a>. <br /></p>
</li>
<li><p>Research intern at <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a> (from May 2018 to Aug 2018) <br />
Working on large-scale face recognition.</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
