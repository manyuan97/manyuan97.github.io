<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Manyuan Zhang</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="cv.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Manyuan Zhang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="bioo.jpg" alt="alt text" width="160px" height="160px" />&nbsp;</td>
<td align="left"><p>Staff Researcher at Meituan-M17, Hong Kong<br />
Ph.D. from <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory</a><br /><a href="http://www.ee.cuhk.edu.hk/en-gb/">Department of Electornic Engineering</a> <br /><a href="https://www.cuhk.edu.hk/chinese/index.html">The Chinese University of Hong Kong</a><br />
Email: zhangmanyuan@link.cuhk.edu.hk <br />
<a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=ZYmcm0EAAAAJ">Scholar</a> <a href="cv.pdf">CV</a> <a href="https://github.com/manyuan97">Github</a> <a href="https://www.linkedin.com/in/manyuan-zhang-32997814b/?originalSubdomain=cn">Linkin</a></p>
</td></tr></table>
<h2>About me</h2>
<p>Now, I am a Staff Researcher at Meituan-M17 (<a href="https://zhaopin.meituan.com/web/beidouprogram">北斗计划</a>), Hong Kong. I received my Ph.D. from <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Lab (MMLab)</a>, the Chinese University of Hong Kong, supervised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof.Hongsheng Li</a> and  <a href="https://www.ee.cuhk.edu.hk/~xgwang/">Prof.Xiaogang Wang</a>. And I received my bachelor's degree from  <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a> in 2019.  Previously, I was a Researcher at <a href="https://www.sensetime.com/cn/">SenseTime Research</a>. <br /><br />
During my six years at <a href="https://www.sensetime.com/cn/">SenseTime Research</a>, I was involved in many projects from scratch. We built the <b>most reliable face recognition system in the world</b> at that time (the champion of <a href="https://pages.nist.gov/frvt/reportcards/11/sensetime_005.html">FRVT</a>, <a href="https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/">ICCV MFR</a>), the <b>best video recognition model</b> (the champion of ActivityNet Challenge <a href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Kinetics700</a>), reimplemented the <b>AI of StarCraft2</b> (<a href="https://github.com/opendilab/DI-star">DI-star</a>) from scratch, developed an autonomous driving algorithm based on reinforcement learning (<a href="https://github.com/opendilab/DI-drive">DI-drive</a>), and most recently, the <b>text-to-image AIGC product <a href="https://miaohua.sensetime.com/">SenseMirage</a></b> (DAU exceeded one million within a week for the first time in SenseTime's history, earning a special commendation from the CEO). <br /><br />
Currently, at Meituan-M17 Hong Kong, I am involved in the development of the LongCat series foundation models, such as <a href="https://arxiv.org/abs/2509.01322">LongCat-Flash</a>. My work focuses on native multimodality, modeling multiple modalities in a unified autoregressive manner. If you are interested in my work or career, please feel free to contact me. <b>Now Hiring Self-motivated Interns! Providing &gt;1000 H-series GPUs!</b></p>
<h2>News</h2>
<ul>
<li><p>[2026-02] One paper accepted to 3DV 2026 with Best Paper Award Nomination!</p>
</li>
<li><p>[2026-01] Two papers accepted to ICLR 2026.</p>
</li>
<li><p>[2025-08] One paper accepted to EMNLP 2025.</p>
</li>
<li><p>[2025-06] One paper accepted to ICCV 2025.</p>
</li>
<li><p>[2025-05] I successfully defended my PhD thesis and officially became Dr. Zhang!</p>
</li>
<li><p>[2025-03] One paper accepted to CVPR 2025.</p>
</li>
<li><p>[2024-07] Two papers accepted to ECCV2024.</p>
</li>
<li><p>[2024-03] One paper accepted to SIGGRAPH2024.</p>
</li>
<li><p>[2023-07] Two paper accepted to ICCV2023.</p>
</li>
<li><p>[2023-07] I pass the PhD candidate test. </p>
</li>
<li><p>[2023-05] I am invited to be a reviewer for NIPS2023 and ICLR2023.</p>
</li>
<li><p>[2023-02] One paper accepted to CVPR 2023.</p>
</li>
<li><p>[2022-12] I am invited to be a reviewer for CVPR2023 and ICCV2023.</p>
</li>
<li><p>[2022-07] One paper accepted to ECCV 2022.</p>
</li>
<li><p>[2022-04] I am invited to be a reviewer for ECCV2022 and NIPS2022.</p>
</li>
<li><p>[2022-04] I am invited to &rsquo;智东西&rsquo; to give a talk about imitation learning in automatic driving.</p>
</li>
<li><p>[2021-10] We win three championships of <a href="https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/">ICCV 2021 Masked Face Recognition Challenge</a> on <a href="http://iccv21-mfr.com/">glink360k</a> track, <a href="http://iccv21-mfr.com/">unconstrained</a> track and <a href="https://competitions.codalab.org/competitions/32478">Webface260M</a> track. Code and solutions will be released very soon.</p>
</li>
<li><p>[2021-07] We release  <a href="https://github.com/opendilab/DI-drive/">DI-drive</a>, the decision intelligence platform for autonomous driving simulation. I am responsible for the imitation learning part.</p>
</li>
<li><p>[2021-07] One paper accepted to ICCV 2021.</p>
</li>
<li><p>[2021-05] We win the championship of <a href="https://pages.nist.gov/frvt/reportcards/11/sensetime_005.html">NIST FRVT 1:1</a>.</p>
</li>
<li><p>[2020-12] We win the championship of <a href="https://pages.nist.gov/frvt/reportcards/1N/sensetime_005.pdf">NIST FRVT 1:N</a>.</p>
</li>
<li><p>[2020-06] We win 2 championships of <a href="http://activity-net.org/">ActivityNet</a> on the <a href="http://activity-net.org/challenges/2020/tasks/guest_ava.html/">Spatio-temporal Action Localization (AVA)</a> track and the <a href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Trimmed Activity Recognition (Kinetics 700)</a> track. </p>
</li>
<li><p>[2020-06] One paper accepted to ECCV 2020.</p>
</li>
<li><p>[2020-04] We release the <a href="https://github.com/Sense-X/X-Temporal/">X-Temporal</a> for easily implement SOTA video understanding methods with PyTorch on multiple machines and GPUs.</p>
</li>
<li><p>[2019-10] One paper accepted to ICCV 2019 LFR workshop.</p>
</li>
<li><p>[2019-10] We win the championship of <a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">ICCV19 Multi-Moments in Time (MIT) Challenge</a>.</p>
</li>
<li><p>[2019-10] We win the championship of <a href="https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/">ICCV19 Lightweight Face Recognition Challenge</a>.</p>
</li>
</ul>
<h3>Challenge Awards</h3>
<ul>
<li><p>Won the <b>1th</b> place in CVPR21 Masked Face Recognition Challenge (<a href="https://www.face-benchmark.org/challenge.html/">WebFace260M</a>, <a href="https://insightface.ai/mfr21/">InsightFace Unconstrained</a> and 
<a href="https://insightface.ai/mfr21">InsightFace glint360k</a> track)</p>
</li>
<li><p>Won the <b>1th</b> place in CVPR20 ActivityNet Challenge (<a href="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html/">Kinetics700</a> track and  <a href="https://www.youtube.com/watch?v=zJPEmG3LCH4&amp;list=PLw6H4u-XW8siSxqdRVcD5aBn3OTuA7M7x/">AVA track</a>)</p>
</li>
<li><p>Won the <b>1th</b> place in NIST FRVT held by US government (<a href="https://github.com/usnistgov/frvt/blob/nist-pages/reports/11/frvt_11_report_2022_07_29.pdf">1:1 Verification</a> and <a href="https://github.com/usnistgov/frvt/blob/nist-pages/reports/1N/frvt_1N_report_2022_07_28.pdf/">1:N Identification</a>)</p>
</li>
<li><p>Won the <b>1th</b> place in <a href="http://moments.csail.mit.edu/results2019.html/">ICCV19 Multi-Moments in Time (MIT) Challenge</a></p>
</li>
<li><p>Won the <b>1th</b> place in <a href="https://insightface.ai/lfr19/">ICCV19 Lightweight Face Recognition Challenge</a></p>
</li>
</ul>
<h3>Technical Report </h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2509.01322">LongCat-Flash Technical Report</a> <br />
Meituan LongCat Team (including <b>Manyuan Zhang</b>), et al.</p>
</li>
<li><p><a href="Towards">Large-scale Masked Face Recognition</a> (Top-1 Solution) <br />
<b>Manyuan Zhang</b>, Bingqi Ma, Guanglu Song, Yunxiao Wang, Hongsheng Li, Yu Liu</p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.09116/">1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020</a>  (Top-1 Solution) <br />
Siyu Chen, Junting Pan, Guanglu Song, <b>Manyuan Zhang</b>, Hao Shao, Ziyi Lin, Jing Shao, Hongsheng Li, Yu Liu </p>
</li>
<li><p><a href="https://arxiv.org/abs/2003.05837/">Top-1 Solution of Multi-Moments in Time Challenge 2019</a>  (Top-1 Solution) <br />
<b>Manyuan Zhang</b>, Hao Shao, Guanglu Song, Yu Liu, Junjie Yan</p>
</li>
</ul>
<h3>Preprint Papers</h3>
<p>*equal contribution <sup>+</sup>project lead/corresponding author</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2601.22154">Exploring Reasoning Reward Model for Agents</a> <br />
Kaixuan Fan, Kaituo Feng, <b>Manyuan Zhang</b><sup>+</sup>, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2512.08294">OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</a> <br />
Yexin Liu, <b>Manyuan Zhang</b><sup>+</sup>, Yueze Wang, Hongyu Li, Dian Zheng, Weiming Zhang, Changsheng Lu, Xunliang Cai, Yan Feng, Peng Pei, Harry Yang</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2512.05965">EditThinker: Unlocking Iterative Reasoning for Any Image Editor</a> <br />
Hongyu Li, <b>Manyuan Zhang</b><sup>+</sup>, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2512.03043">OneThinker: All-in-one Reasoning Model for Image and Video</a> <br />
Kaituo Feng, <b>Manyuan Zhang</b><sup>+</sup>, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, Yan Feng, Peng Pei, Xunliang Cai, Xiangyu Yue</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2511.22663">Architecture Decoupling is Not All You Need for Unified Multimodal Model</a> <br />
Dian Zheng, <b>Manyuan Zhang</b><sup>+</sup>, Hongyu Li, Kai Zou, Hongbo Liu, Ziyu Guo, Kaituo Feng, Yexin Liu, Ying Luo, Yan Feng, Peng Pei, Xunliang Cai, Hongsheng Li</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2511.16671">Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a> <br />
Ziyu Guo, Renrui Zhang, Hongyu Li, <b>Manyuan Zhang</b><sup>+</sup>, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2510.18632">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a> <br />
Zhangquan Chen, <b>Manyuan Zhang</b><sup>+</sup>, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2510.11718">CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-driven Images</a> <br />
Chengqi Duan, Kaiyue Sun, Rongyao Fang, <b>Manyuan Zhang</b><sup>+</sup>, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu</p>
</li>
</ul>
<h3>Recent Publications </h3>
<p>*equal contribution</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2510.22706">IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</a> <br />
Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, <b>Manyuan Zhang</b>, Gang Yu, Dingwen Zhang, Ziwei Liu <br />
2026 International Conference on Learning Representations (ICLR)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2510.08457">Ares: Multimodal Adaptive Reasoning via Difficulty-aware Token-level Entropy Shaping</a> <br />
Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, <b>Manyuan Zhang</b>, Jiayu Chen, Song Guo, Nanyun Peng <br />
2026 International Conference on Learning Representations (ICLR)</p>
</li>
</ul>
<ul>
<li><p>CTR3D: Cross-view Token Reduction for Dense Multi-view Generation <br />
Kunming Luo, Hongyu Yan, Yuan Liu, Zihao Zhang, <b>Manyuan Zhang</b>, Wenping Wang, Ping Tan <br />
2026 International Conference on 3D Vision (3DV) <font color="red">(Best Paper Award Nomination)</font></p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2509.05657">LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</a> <br />
Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zheng, Weikang Shi, <b>Manyuan Zhang</b>, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li <br />
2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2503.21758">Lumina-image 2.0: A unified and efficient image generative framework</a> <br />
Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, <b>Manyuan Zhang</b>, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, Peng Gao <br />
2025 International Conference on Computer Vision (ICCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2501.13926">Let's Verify and Reinforce Image Generation Step by Step</a> <br />
Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Ziyu Guo, Haoquan Zhang, <b>Manyuan Zhang</b>, Jiaming Liu, Peng Gao, Hongsheng Li <br />
2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
</ul>
<ul>
<li><p><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05837.pdf">Three Things We Need to Know About Transferring Stable Diffusion to Visual Dense Prediciton Tasks</a> <br />
<b>Manyuan Zhang</b>, Guanglu Song, Yu Liu, Hongsheng Li <br />
2024 European Conference on Computer Vision (ECCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.15977">Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</a> <br />
Xiaoshi Wu, Yiming Hao, <b>Manyuan Zhang</b>, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li <br />
2024 European Conference on Computer Vision (ECCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.15977">Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</a> <br />
Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, <b>Manyuan Zhang</b>, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li
2024 ACM SIGGRAPH</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2310.15955">Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection</a> <br />
<b>Manyuan Zhang</b>, Guanglu Song, Yu Liu, Hongsheng Li <br />
2023  International Conference on Computer Vision (ICCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2303.08340">VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation</a> <br />
Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, <b>Manyuan Zhang</b>, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li <br />
2023  International Conference on Computer Vision (ICCV)</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2303.01237">FlowFormer<tt></tt>: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation</a> <br />
Xiaoyu Shi, Zhaoyang Huang, Dasong Li, <b>Manyuan Zhang</b>, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li <br />
2022 The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) </p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.13600/">Towards Robust Face Recognition with Comprehensive Search</a> <br />
<b>Manyuan Zhang</b>, Guanglu Song, Yu Liu, Hongsheng Li <br />
2022 European Conference on Computer Vision (ECCV)</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Switchable_K-Class_Hyperplanes_for_Noise-Robust_Representation_Learning_ICCV_2021_paper.pdf/">Switchable K-class Hyperplanes for Noise-robust Representation Learning</a> <br /> 
Boxiao Liu, Guanglu Song, <b>Manyuan Zhang</b>, Haihang You, Yu Liu <br />
2021 International Conference on Computer Vision (ICCV)</p>
</li>
<li><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550001.pdf/">Discriminability Distillation in Group Representation Learning</a> <br />
<b>Manyuan Zhang</b>, Guanglu Song, Hang Zhou, Yu Liu <br />
2020 European Conference on Computer Vision (ECCV)</p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Liu_Towards_Flops-Constrained_Face_Recognition_ICCVW_2019_paper.pdf/">Towards Flops-constrained Face Recognition</a> <br />
Yu Liu*, Guanglu Song*, <b>Manyuan Zhang*</b>, Jihao Liu*, Yucong Zhou, Junjie Yan <br />
2019 ICCV Lightweight Face Recognition Challenge &amp; Workshop</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/8486609">Tensor sensing for RF tomographic imaging</a> <br />
Tao Deng, Feng Qian, Xiao-Yang Liu, <b>Manyuan Zhang</b>, Anwar Walid <br /> 
2018 IEEE International Conference on Multimedia and Expo (ICME) </p>
</li>
<li><p><a href="https://arxiv.org/pdf/1803.10943.pdf/">Privacy-preserving sensory data recovery</a> <br />
Cai Chen, <b>Manyuan Zhang</b>, Huanzhi Zhang, Zhenyun Huang, Yong Li <br />
2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications <br /></p>
</li>
</ul>
<h3>Selected Projects</h3>
<ul>
<li><p><a href="https://github.com/Sense-X/X-Temporal/">X-Xemporal</a> <br />
Easily implement SOTA video understanding methods with PyTorch on multiple machines and GPU.</p>
</li>
<li><p><a href="https://github.com/opendilab/DI-drive/">DI-drive</a> <br /> 
Decision Intelligence Platform for Autonomous Driving simulation.</p>
</li>
</ul>
<h3>Working Experience</h3>
<ul>
<li><p>Research intern at <a href="https://www.sensetime.com/cn/">SenseTime Research</a> (since Feb 2019) <br />
Working on large-scale face recognition and video understanding with <a href="http://liuyu.us/">Yu Liu</a> and <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&amp;hl=zh-CN/">Guanglu Song</a></p>
</li>
<li><p>Research intern at <a href="https://en.megvii.com/megvii_research/">Megvii Research</a> (from Aug 2018 to Feb 2019) <br />
Working on style transfer with <a href="http://www.liushuaicheng.org/">Shuaicheng Liu</a>. <br /></p>
</li>
<li><p>Research intern at <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a> (from May 2018 to Aug 2018) <br />
Working on large-scale face recognition.</p>
</li>
</ul>
</td>
</tr>
</table>
<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=A5xDGc_wZOf0OFNs-6UudWghfSkX4Cw10a4tMaBBNxE&cl=ffffff&w=a"></script>
</body>
</html>
